{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "parameters = {\n",
    "    \"num_class\": 8,\n",
    "    \"time\": str(datetime.now()).replace(\" \", \"_\"),\n",
    "    \"seed\": 1111,\n",
    "    # Hyperparameters\n",
    "    \"model_name\": 'BERT',\n",
    "    \"config\": 'bert-base-uncased',\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"epochs\": 3,\n",
    "    \"max_len\": 512,\n",
    "    \"batch_size\": 16,\n",
    "    \"dropout\": 0.1,\n",
    "    \"activation\": 'Prelu',\n",
    "    \"hidden_dim\": 384,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         tweet_id identification\n",
      "0        0x28cc61           test\n",
      "1        0x29e452          train\n",
      "2        0x2b3819          train\n",
      "3        0x2db41f           test\n",
      "4        0x2a2acc          train\n",
      "...           ...            ...\n",
      "1867530  0x227e25          train\n",
      "1867531  0x293813          train\n",
      "1867532  0x1e1a7e          train\n",
      "1867533  0x2156a5          train\n",
      "1867534  0x2bb9d2          train\n",
      "\n",
      "[1867535 rows x 2 columns]\n",
      "(1867535, 2)\n",
      "========================================\n",
      "         tweet_id       emotion\n",
      "0        0x3140b1       sadness\n",
      "1        0x368b73       disgust\n",
      "2        0x296183  anticipation\n",
      "3        0x2bd6e1           joy\n",
      "4        0x2ee1dd  anticipation\n",
      "...           ...           ...\n",
      "1455558  0x38dba0           joy\n",
      "1455559  0x300ea2           joy\n",
      "1455560  0x360b99          fear\n",
      "1455561  0x22eecf           joy\n",
      "1455562  0x2fb282  anticipation\n",
      "\n",
      "[1455563 rows x 2 columns]\n",
      "(1455563, 2)\n",
      "========================================\n",
      "              id   emotion\n",
      "0       0x2c7743  surprise\n",
      "1       0x2c1eed  surprise\n",
      "2       0x2826ea  surprise\n",
      "3       0x356d9a  surprise\n",
      "4       0x20fd95  surprise\n",
      "...          ...       ...\n",
      "411967  0x351857  surprise\n",
      "411968  0x2c028e  surprise\n",
      "411969  0x1f2430  surprise\n",
      "411970  0x2be24e  surprise\n",
      "411971  0x35802a  surprise\n",
      "\n",
      "[411972 rows x 2 columns]\n",
      "========================================\n",
      "Show ids of train and test\n",
      "\n",
      "1455563\n",
      "411972\n",
      "1867535\n",
      "After expand the tweet_id, tweet_hashtag...\n",
      "\n",
      "After saperate train and test:\n",
      "\n",
      "(1455563, 8)\n",
      "(411972, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "folder_name = 'dm-2024-isa-5810-lab-2-homework'\n",
    "data_identification = pd.read_csv(folder_name + '/data_identification.csv')\n",
    "emotion = pd.read_csv(folder_name + '/emotion.csv')\n",
    "sample_submission = pd.read_csv(folder_name + '/sampleSubmission.csv')\n",
    "\n",
    "print(data_identification)\n",
    "print(data_identification.shape)\n",
    "print(f\"{'='*40}\")\n",
    "print(emotion)\n",
    "print(emotion.shape)\n",
    "print(f\"{'='*40}\")\n",
    "print(sample_submission)\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "df_twitter = pd.read_json(folder_name + '/tweets_DM.json', lines=True)\n",
    "train_ids = data_identification[data_identification['identification'] == 'train']['tweet_id'].tolist()\n",
    "test_ids = data_identification[data_identification['identification'] == 'test']['tweet_id'].tolist()\n",
    "\n",
    "print(\"Show ids of train and test\\n\")\n",
    "print(len(train_ids))\n",
    "print(len(test_ids))\n",
    "print(len(train_ids) + len(test_ids))\n",
    "\n",
    "df_twitter_expanded = pd.json_normalize(df_twitter['_source'])\n",
    "\n",
    "print(\"After expand the tweet_id, tweet_hashtag...\\n\")\n",
    "df_twitter['tweet_id'] = df_twitter_expanded['tweet.tweet_id']\n",
    "df_twitter['text'] = df_twitter_expanded['tweet.text']\n",
    "df_twitter['hash_tags'] = df_twitter_expanded['tweet.hashtags']\n",
    "\n",
    "df_twitter_train = df_twitter[df_twitter['tweet_id'].isin(train_ids)]\n",
    "df_twitter_test = df_twitter[df_twitter['tweet_id'].isin(test_ids)]\n",
    "\n",
    "print(\"After saperate train and test:\\n\")\n",
    "print(df_twitter_train.shape)\n",
    "print(df_twitter_test.shape)\n",
    "\n",
    "df_twitter_train = pd.merge(df_twitter_train, emotion, on='tweet_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use label encoder to turn word label into number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "Encoded Labels: [1 5 3 4]\n",
      "Mapping: {'anger': 0, 'anticipation': 1, 'disgust': 2, 'fear': 3, 'joy': 4, 'sadness': 5, 'surprise': 6, 'trust': 7}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_source</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hash_tags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Snapchat'], 'tweet_id...</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['freepress', 'TrumpLeg...</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['authentic', 'LaughOut...</td>\n",
       "      <td>2015-06-11 04:44:05</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>joy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2c91...</td>\n",
       "      <td>2015-08-18 02:30:07</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _score          _index                                            _source  \\\n",
       "0     391  hashtag_tweets  {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
       "1     433  hashtag_tweets  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
       "2     376  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
       "3     120  hashtag_tweets  {'tweet': {'hashtags': ['authentic', 'LaughOut...   \n",
       "4    1021  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x2c91...   \n",
       "\n",
       "            _crawldate   _type  tweet_id  \\\n",
       "0  2015-05-23 11:42:47  tweets  0x376b20   \n",
       "1  2016-01-28 04:52:09  tweets  0x2d5350   \n",
       "2  2016-01-24 23:53:05  tweets  0x1cd5b0   \n",
       "3  2015-06-11 04:44:05  tweets  0x1d755c   \n",
       "4  2015-08-18 02:30:07  tweets  0x2c91a8   \n",
       "\n",
       "                                                text  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2                Now ISSA is stalking Tasha 😂😂😂 <LH>   \n",
       "3  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "4       Still waiting on those supplies Liscus. <LH>   \n",
       "\n",
       "                       hash_tags       emotion  label  \n",
       "0                     [Snapchat]  anticipation      1  \n",
       "1  [freepress, TrumpLegacy, CNN]       sadness      5  \n",
       "2                             []          fear      3  \n",
       "3      [authentic, LaughOutLoud]           joy      4  \n",
       "4                             []  anticipation      1  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "emotion_label = df_twitter_train['emotion']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(emotion_label)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "\n",
    "emotion_label = label_encoder.fit_transform(df_twitter_train['emotion'])\n",
    "\n",
    "# 檢查轉換結果\n",
    "print(\"Encoded Labels:\", emotion_label[:4])\n",
    "print(\"Mapping:\", dict(zip(label_encoder.classes_, range(len(label_encoder.classes_)))))\n",
    "\n",
    "df_twitter_train['label'] = emotion_label\n",
    "df_twitter_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1164450, 10)\n",
      "(291113, 10)\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = train_test_split(df_twitter_train, random_state=42, test_size=0.2)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text  label\n",
      "834097  For those of you who have followed me, thanks ...      4\n",
      "355739  When you have to take a day off from work in o...      5\n",
      "625638  Wherever you are; be all there. That’s how <LH...      4\n",
      "678647  @DesiMountaineer  Would be nice for all the PS...      4\n",
      "441397  69 The moments in your life are only once #Lif...      7\n",
      "                                                      text  label\n",
      "970345   Been a #week now #since I <LH> my #Mom. I #mis...      6\n",
      "1145883  Follow our Librarian, Ms. Bird 🐦 for more info...      4\n",
      "468264   Wonder if the guys who skate in Foxboro over t...      4\n",
      "949718   @vanillablack1 Bloody <LH> puts it mildly, wil...      4\n",
      "982592   Beat the Dolphins next week and we are back to...      4\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.DataFrame({\"text\": train_data['text'], \"label\": train_data['label']})\n",
    "val_data = pd.DataFrame({\"text\": val_data['text'], \"label\": val_data['label']})\n",
    "print(train_data.head())\n",
    "print(val_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "\n",
    "config_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as Fun\n",
    "\n",
    "# Using Dataset to build DataLoader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mode, df, specify, args):\n",
    "        assert mode in [\"train\", \"val\", \"test\"]  # 一般會切三份\n",
    "        self.mode = mode\n",
    "        self.df = df\n",
    "        self.specify = specify # specify column of data (the column U use for predict)\n",
    "        if self.mode != 'test':\n",
    "          self.label = df['label']\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(args[\"config\"])\n",
    "        self.max_len = args[\"max_len\"]\n",
    "        self.num_class = args[\"num_class\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    # transform text to its number\n",
    "    def tokenize(self,input_text):\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            input_text,\n",
    "            max_length = self.max_len,\n",
    "            truncation = True,\n",
    "            padding = 'max_length'\n",
    "        )\n",
    "        ids = inputs['input_ids'] # (512)\n",
    "        mask = inputs['attention_mask'] # (512)\n",
    "        token_type_ids = inputs[\"token_type_ids\"] # (512)\n",
    "        \n",
    "        return ids, mask, token_type_ids\n",
    "\n",
    "    # get single data\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sentence = str(self.df[self.specify][index])\n",
    "        ids, mask, token_type_ids = self.tokenize(sentence)\n",
    "\n",
    "        if self.mode == \"test\":\n",
    "            return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \\\n",
    "                torch.tensor(token_type_ids, dtype=torch.long)\n",
    "        else:\n",
    "            return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \\\n",
    "                torch.tensor(token_type_ids, dtype=torch.long), torch.tensor(self.label.iloc[index], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "# load training data\n",
    "# 你可以先 sample 部分資料去跑模型，有助於快速調整模型架構，畢竟資料愈多跑愈久\n",
    "train_df = train_data.sample(4000, random_state=parameters['seed']).reset_index(drop=True)\n",
    "train_dataset = CustomDataset('train', train_df, 'text', parameters)\n",
    "train_loader = DataLoader(train_dataset, batch_size=parameters['batch_size'], shuffle=True)\n",
    "\n",
    "# load validation data\n",
    "val_df = val_data.sample(500, random_state=parameters['seed']).reset_index(drop=True)\n",
    "val_dataset = CustomDataset('val', val_df, 'text', parameters)\n",
    "val_loader = DataLoader(val_dataset, batch_size=parameters['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "# define different activation function\n",
    "def get_activation(activation):\n",
    "    if activation == 'Prelu':\n",
    "        return nn.PReLU()\n",
    "    elif activation == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif activation == 'gelu':\n",
    "        return nn.GELU()\n",
    "    elif activation == 'LeakyReLU':\n",
    "        return nn.LeakyReLU()\n",
    "    else:\n",
    "        return nn.Tanh()\n",
    "# Dense Layer\n",
    "# It is composed of linear, dropout, and activation layers.\n",
    "class Dense(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate, activation='tanh'):\n",
    "        super(Dense, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.activation = get_activation(activation) # default tanh\n",
    "        nn.init.xavier_uniform_(self.hidden_layer.weight) # you also can change the initialize method\n",
    "    def forward(self, inputs):\n",
    "        logits = self.hidden_layer(inputs)\n",
    "        logits = self.dropout(logits)\n",
    "        logits = self.activation(logits)\n",
    "        return logits\n",
    "# multi-layers\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "# Hidden Layers\n",
    "# It means there are many dense layers with the same dimension\n",
    "class HiddenLayers(nn.Module):\n",
    "    def __init__(self, dense_layer, num_layers):\n",
    "        super(HiddenLayers, self).__init__()\n",
    "        self.hidden_layers = _get_clones(dense_layer, num_layers)\n",
    "    def forward(self, output):\n",
    "        for layer in self.hidden_layers:\n",
    "            output = layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "# BERT Model\n",
    "class BertClassifier(BertPreTrainedModel):\n",
    "    def __init__(self, config, args):\n",
    "        super(BertClassifier, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.num_labels = args[\"num_class\"]\n",
    "        self.dense = Dense(config.hidden_size, args[\"hidden_dim\"], args[\"dropout\"], args[\"activation\"])\n",
    "        self.classifier = Dense(args[\"hidden_dim\"], self.num_labels, args[\"dropout\"], args[\"activation\"])\n",
    "        self.init_weights()\n",
    "    # forward function, data in the model will do this\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None,\n",
    "                head_mask=None, inputs_embeds=None, labels=None, output_attentions=None,\n",
    "                output_hidden_states=None, return_dict=None):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        # bert output\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "        '''\n",
    "        outputs.keys() -> odict_keys(['last_hidden_state', 'pooler_output'])\n",
    "        outs.last_hidden_state.shape -> torch.Size([batch_size, 512, 768])\n",
    "        outs.pooler_output.shape -> torch.Size([batch_size, 768])\n",
    "        '''\n",
    "        # get its [CLS] logits\n",
    "        pooled_output = outputs[1] # (batch_size, 768)\n",
    "        # add dense layer\n",
    "        pooled_output = self.dense(pooled_output) # (batch_size, 384)\n",
    "        # add linear classifier\n",
    "        logits = self.classifier(pooled_output) # (batch_size, 2)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score# get predict result\n",
    "\n",
    "def get_pred(logits):\n",
    "    y_pred = torch.argmax(logits, dim = 1)\n",
    "    return y_pred\n",
    "\n",
    "# calculate confusion metrics\n",
    "def cal_metrics(pred, ans, method):\n",
    "    '''\n",
    "    Parameter\n",
    "    ---------\n",
    "    pred: [list], predict class\n",
    "    ans: [list], true class\n",
    "    method: 'micro', 'weighted', 'macro'. # 如果有多分類的話計算上會有差別\n",
    "    ---------\n",
    "    '''\n",
    "    if pred.get_device() != 'cpu':\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "    if ans.get_device() != 'cpu':\n",
    "        ans = ans.detach().cpu().numpy()\n",
    "    # 將 zero_division 設為 0，表示當所有預測皆錯誤時，將結果視為 0 \n",
    "    rec = recall_score(pred, ans, average=method, zero_division=0)\n",
    "    f1 = f1_score(pred, ans, average=method, zero_division=0)\n",
    "    prec = precision_score(pred, ans, average=method, zero_division=0)\n",
    "    acc = accuracy_score(pred, ans)\n",
    "    return acc, f1, rec, prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.activation.weight', 'classifier.hidden_layer.bias', 'classifier.hidden_layer.weight', 'dense.activation.weight', 'dense.hidden_layer.bias', 'dense.hidden_layer.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m loss_fct \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mECrossntropyLoss() \u001b[38;5;66;03m# we use cross entrophy loss\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m## You can custom your optimizer (e.g. SGD .etc) ##\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# we use Adam here\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hans\\anaconda3\\envs\\DataMining\\Lib\\site-packages\\transformers\\modeling_utils.py:3157\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   3153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3156\u001b[0m         )\n\u001b[1;32m-> 3157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hans\\anaconda3\\envs\\DataMining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hans\\anaconda3\\envs\\DataMining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hans\\anaconda3\\envs\\DataMining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hans\\anaconda3\\envs\\DataMining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hans\\anaconda3\\envs\\DataMining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hans\\anaconda3\\envs\\DataMining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = BertClassifier.from_pretrained(parameters['config'], parameters).to(device)\n",
    "loss_fct = nn.ECrossntropyLoss() # we use cross entrophy loss\n",
    "\n",
    "## You can custom your optimizer (e.g. SGD .etc) ##\n",
    "# we use Adam here\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=parameters['learning_rate'], betas=(0.9, 0.999), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# evaluate dataloader\n",
    "def evaluate(model, data_loader, device):\n",
    "    val_loss, val_acc, val_f1, val_rec, val_prec = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    step_count = 0\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            ids, masks, token_type_ids, labels = [t.to(device) for t in data]\n",
    "\n",
    "            logits = model(input_ids = ids,\n",
    "                    token_type_ids = token_type_ids,\n",
    "                    attention_mask = masks)\n",
    "            acc, f1, rec, prec = cal_metrics(get_pred(logits), labels, 'macro')\n",
    "            loss = loss_fct(logits, labels) # 直接丟就好，不用特意做轉換（但如果非二分類，需考慮 one-hot 標籤的轉換）\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_acc += acc\n",
    "            val_f1 += f1\n",
    "            val_rec += rec\n",
    "            val_prec += prec\n",
    "            step_count+=1\n",
    "\n",
    "        val_loss = val_loss / step_count\n",
    "        val_acc = val_acc / step_count\n",
    "        val_f1 = val_f1 / step_count\n",
    "        val_rec = val_rec / step_count\n",
    "        val_prec = val_prec / step_count\n",
    "\n",
    "    return val_loss, val_acc, val_f1, val_rec, val_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to path\n",
    "def save_checkpoint(save_path, model):\n",
    "    if save_path == None:\n",
    "        return\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "# load model from path\n",
    "def load_checkpoint(load_path, model, device):\n",
    "    if load_path==None:\n",
    "        return\n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'\\nModel loaded from <== {load_path}')\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(model, train_loader, val_loader, optimizer, args, device):\n",
    "\n",
    "    metrics = ['loss', 'acc', 'f1', 'rec', 'prec']\n",
    "    mode = ['train_', 'val_']\n",
    "    record = {s+m :[] for s in mode for m in metrics}\n",
    "\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(args[\"epochs\"]):\n",
    "\n",
    "        st_time = time.time()\n",
    "        train_loss, train_acc, train_f1, train_rec, train_prec = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        step_count = 0\n",
    "\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "\n",
    "            ids, masks, token_type_ids, labels = [t.to(device) for t in data]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            logits = model(input_ids = ids,\n",
    "                    token_type_ids = token_type_ids,\n",
    "                    attention_mask = masks)\n",
    "\n",
    "            acc, f1, rec, prec = cal_metrics(get_pred(logits), labels, 'macro')\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += acc\n",
    "            train_f1 += f1\n",
    "            train_rec += rec\n",
    "            train_prec += prec\n",
    "            step_count += 1\n",
    "\n",
    "        val_loss, val_acc, val_f1, val_rec, val_prec = evaluate(model, val_loader, device)\n",
    "\n",
    "        train_loss = train_loss / step_count\n",
    "        train_acc = train_acc / step_count\n",
    "        train_f1 = train_f1 / step_count\n",
    "        train_rec = train_rec / step_count\n",
    "        train_prec = train_prec / step_count\n",
    "\n",
    "        print('[epoch %d] cost time: %.4f s'%(epoch + 1, time.time() - st_time))\n",
    "        print('         loss     acc     f1      rec    prec')\n",
    "        print('train | %.4f, %.4f, %.4f, %.4f, %.4f'%(train_loss, train_acc, train_f1, train_rec, train_prec))\n",
    "        print('val  | %.4f, %.4f, %.4f, %.4f, %.4f\\n'%(val_loss, val_acc, val_f1, val_rec, val_prec))\n",
    "\n",
    "        # record training metrics of each training epoch\n",
    "        record['train_loss'].append(train_loss)\n",
    "        record['train_acc'].append(train_acc)\n",
    "        record['train_f1'].append(train_f1)\n",
    "        record['train_rec'].append(train_rec)\n",
    "        record['train_prec'].append(train_prec)\n",
    "    \n",
    "        record['val_loss'].append(val_loss)\n",
    "        record['val_acc'].append(val_acc)\n",
    "        record['val_f1'].append(val_f1)\n",
    "        record['val_rec'].append(val_rec)\n",
    "        record['val_prec'].append(val_prec)\n",
    "\n",
    "    # save model\n",
    "    save_checkpoint(args[\"model_name\"] + '_' + args[\"time\"].split('_')[0] + '.pt', model)\n",
    "\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# draw the learning curve\n",
    "def draw_pic(record, name, img_save=False, show=False):\n",
    "    x_ticks = range(1, parameters['epochs']+1)\n",
    "\n",
    "    plt.figure(figsize=(6, 3))\n",
    "\n",
    "    plt.plot(x_ticks, record['train_'+name], '-o', color='lightskyblue',\n",
    "             markeredgecolor=\"teal\", markersize=3, markeredgewidth=1, label = 'Train')\n",
    "    plt.plot(x_ticks, record['val_'+name], '-o', color='pink',\n",
    "             markeredgecolor=\"salmon\", markersize=3, markeredgewidth=1, label = 'Val')\n",
    "    plt.grid(color='lightgray', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.title('Model', fontsize=14)\n",
    "    plt.ylabel(name, fontsize=12)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.xticks(x_ticks, fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(loc='lower right' if not name.lower().endswith('loss') else 'upper right')\n",
    "\n",
    "    # define saved figure or not\n",
    "    if img_save:\n",
    "        plt.savefig(name+'.png', transparent=False, dpi=300)\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# draw all metrics figure\u001b[39;00m\n\u001b[0;32m      4\u001b[0m draw_pic(history, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, img_save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[52], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, optimizer, args, device)\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m---> 19\u001b[0m     ids, masks, token_type_ids, labels \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     23\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(input_ids \u001b[38;5;241m=\u001b[39m ids,\n\u001b[0;32m     24\u001b[0m             token_type_ids \u001b[38;5;241m=\u001b[39m token_type_ids,\n\u001b[0;32m     25\u001b[0m             attention_mask \u001b[38;5;241m=\u001b[39m masks)\n",
      "Cell \u001b[1;32mIn[52], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m---> 19\u001b[0m     ids, masks, token_type_ids, labels \u001b[38;5;241m=\u001b[39m [\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     23\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(input_ids \u001b[38;5;241m=\u001b[39m ids,\n\u001b[0;32m     24\u001b[0m             token_type_ids \u001b[38;5;241m=\u001b[39m token_type_ids,\n\u001b[0;32m     25\u001b[0m             attention_mask \u001b[38;5;241m=\u001b[39m masks)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "history = train(model, train_loader, val_loader, optimizer, parameters, device)\n",
    "\n",
    "# draw all metrics figure\n",
    "draw_pic(history, 'loss', img_save=True, show=False)\n",
    "draw_pic(history, 'acc', img_save=True, show=False)\n",
    "draw_pic(history, 'f1', img_save=True, show=False)\n",
    "draw_pic(history, 'rec', img_save=True, show=False)\n",
    "draw_pic(history, 'prec', img_save=True, show=False)\n",
    "\n",
    "files = []\n",
    "files.append('loss.png')\n",
    "files.append('acc.png')\n",
    "files.append('f1.png')\n",
    "files.append('rec.png')\n",
    "files.append('prec.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum()\n",
    "\n",
    "# predict a single sentence\n",
    "def predict_one(query, model):\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained(parameters['config'])\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    inputs = tokenizer.encode_plus(\n",
    "            query,\n",
    "            max_length = parameters['max_len'],\n",
    "            truncation = True,\n",
    "            padding = 'max_length',\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
    "\n",
    "    # forward pass\n",
    "    logits = model(input_ids, attention_mask, token_type_ids)\n",
    "    probs = Softmax(logits) # get each class-probs\n",
    "    label_index = torch.argmax(probs[0], dim=0)\n",
    "    pred = label_index.item()\n",
    "\n",
    "  return probs, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.activation.weight', 'classifier.hidden_layer.bias', 'classifier.hidden_layer.weight', 'dense.activation.weight', 'dense.hidden_layer.bias', 'dense.hidden_layer.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\hans\\AppData\\Local\\Temp\\ipykernel_22504\\1987939282.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(load_path, map_location=device)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './bert.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# You can load the model from the existing result\u001b[39;00m\n\u001b[0;32m      2\u001b[0m init_model \u001b[38;5;241m=\u001b[39m BertClassifier\u001b[38;5;241m.\u001b[39mfrom_pretrained(parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m], parameters) \u001b[38;5;66;03m# build an initial model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./bert.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# and load the weight of model from specify file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[47], line 12\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[1;34m(load_path, model, device)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_path\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel loaded from <== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n",
      "File \u001b[1;32mc:\\Users\\hans\\anaconda3\\envs\\DataMining\\Lib\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\hans\\anaconda3\\envs\\DataMining\\Lib\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\hans\\anaconda3\\envs\\DataMining\\Lib\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './bert.pt'"
     ]
    }
   ],
   "source": [
    "# You can load the model from the existing result\n",
    "init_model = BertClassifier.from_pretrained(parameters['config'], parameters) # build an initial model\n",
    "model = load_checkpoint('./bert.pt', init_model, device).to(device) # and load the weight of model from specify file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
