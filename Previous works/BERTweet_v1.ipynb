{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         tweet_id identification\n",
      "0        0x28cc61           test\n",
      "1        0x29e452          train\n",
      "2        0x2b3819          train\n",
      "3        0x2db41f           test\n",
      "4        0x2a2acc          train\n",
      "...           ...            ...\n",
      "1867530  0x227e25          train\n",
      "1867531  0x293813          train\n",
      "1867532  0x1e1a7e          train\n",
      "1867533  0x2156a5          train\n",
      "1867534  0x2bb9d2          train\n",
      "\n",
      "[1867535 rows x 2 columns]\n",
      "(1867535, 2)\n",
      "========================================\n",
      "         tweet_id       emotion\n",
      "0        0x3140b1       sadness\n",
      "1        0x368b73       disgust\n",
      "2        0x296183  anticipation\n",
      "3        0x2bd6e1           joy\n",
      "4        0x2ee1dd  anticipation\n",
      "...           ...           ...\n",
      "1455558  0x38dba0           joy\n",
      "1455559  0x300ea2           joy\n",
      "1455560  0x360b99          fear\n",
      "1455561  0x22eecf           joy\n",
      "1455562  0x2fb282  anticipation\n",
      "\n",
      "[1455563 rows x 2 columns]\n",
      "(1455563, 2)\n",
      "========================================\n",
      "              id   emotion\n",
      "0       0x2c7743  surprise\n",
      "1       0x2c1eed  surprise\n",
      "2       0x2826ea  surprise\n",
      "3       0x356d9a  surprise\n",
      "4       0x20fd95  surprise\n",
      "...          ...       ...\n",
      "411967  0x351857  surprise\n",
      "411968  0x2c028e  surprise\n",
      "411969  0x1f2430  surprise\n",
      "411970  0x2be24e  surprise\n",
      "411971  0x35802a  surprise\n",
      "\n",
      "[411972 rows x 2 columns]\n",
      "========================================\n",
      "Show ids of train and test\n",
      "\n",
      "1455563\n",
      "411972\n",
      "1867535\n",
      "After expand the tweet_id, tweet_hashtag...\n",
      "\n",
      "After saperate train and test:\n",
      "\n",
      "(1455563, 8)\n",
      "(411972, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# Environment = 1 # Kaggle\n",
    "Environment = 2 # Local\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if Environment == 1:\n",
    "    folder_name = '/kaggle/input'\n",
    "elif Environment == 2:\n",
    "    folder_name = 'dm-2024-isa-5810-lab-2-homework'\n",
    "\n",
    "data_identification = pd.read_csv(folder_name + '/data_identification.csv')\n",
    "emotion = pd.read_csv(folder_name + '/emotion.csv')\n",
    "sample_submission = pd.read_csv(folder_name + '/sampleSubmission.csv')\n",
    "\n",
    "print(data_identification)\n",
    "print(data_identification.shape)\n",
    "print(f\"{'='*40}\")\n",
    "print(emotion)\n",
    "print(emotion.shape)\n",
    "print(f\"{'='*40}\")\n",
    "print(sample_submission)\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "df_twitter = pd.read_json(folder_name + '/tweets_DM.json', lines=True)\n",
    "train_ids_df = data_identification[data_identification['identification'] == 'train'].drop(['identification'], axis=1)\n",
    "test_ids_df = data_identification[data_identification['identification'] == 'test'].drop(['identification'], axis=1)\n",
    "train_ids = data_identification[data_identification['identification'] == 'train']['tweet_id'].tolist()\n",
    "test_ids = data_identification[data_identification['identification'] == 'test']['tweet_id'].tolist()\n",
    "\n",
    "print(\"Show ids of train and test\\n\")\n",
    "print(len(train_ids))\n",
    "print(len(test_ids))\n",
    "print(len(train_ids) + len(test_ids))\n",
    "\n",
    "df_twitter_expanded = pd.json_normalize(df_twitter['_source'])\n",
    "\n",
    "print(\"After expand the tweet_id, tweet_hashtag...\\n\")\n",
    "df_twitter['tweet_id'] = df_twitter_expanded['tweet.tweet_id']\n",
    "df_twitter['text'] = df_twitter_expanded['tweet.text']\n",
    "df_twitter['hash_tags'] = df_twitter_expanded['tweet.hashtags']\n",
    "\n",
    "df_twitter_train = df_twitter[df_twitter['tweet_id'].isin(train_ids)]\n",
    "df_twitter_test = df_twitter[df_twitter['tweet_id'].isin(test_ids)]\n",
    "\n",
    "print(\"After saperate train and test:\\n\")\n",
    "print(df_twitter_train.shape)\n",
    "print(df_twitter_test.shape)\n",
    "\n",
    "df_twitter_train = pd.merge(df_twitter_train, emotion, on='tweet_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_source</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hash_tags</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Snapchat'], 'tweet_id...</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['freepress', 'TrumpLeg...</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['authentic', 'LaughOut...</td>\n",
       "      <td>2015-06-11 04:44:05</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2c91...</td>\n",
       "      <td>2015-08-18 02:30:07</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _score          _index                                            _source  \\\n",
       "0     391  hashtag_tweets  {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
       "1     433  hashtag_tweets  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
       "2     376  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
       "3     120  hashtag_tweets  {'tweet': {'hashtags': ['authentic', 'LaughOut...   \n",
       "4    1021  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x2c91...   \n",
       "\n",
       "            _crawldate   _type  tweet_id  \\\n",
       "0  2015-05-23 11:42:47  tweets  0x376b20   \n",
       "1  2016-01-28 04:52:09  tweets  0x2d5350   \n",
       "2  2016-01-24 23:53:05  tweets  0x1cd5b0   \n",
       "3  2015-06-11 04:44:05  tweets  0x1d755c   \n",
       "4  2015-08-18 02:30:07  tweets  0x2c91a8   \n",
       "\n",
       "                                                text  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "3  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "4       Still waiting on those supplies Liscus. <LH>   \n",
       "\n",
       "                       hash_tags       emotion  \n",
       "0                     [Snapchat]  anticipation  \n",
       "1  [freepress, TrumpLegacy, CNN]       sadness  \n",
       "2                             []          fear  \n",
       "3      [authentic, LaughOutLoud]           joy  \n",
       "4                             []  anticipation  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twitter_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[1 5 3 4 0 7 2 6]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_source</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hash_tags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotion_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Snapchat'], 'tweet_id...</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['freepress', 'TrumpLeg...</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['authentic', 'LaughOut...</td>\n",
       "      <td>2015-06-11 04:44:05</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>joy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2c91...</td>\n",
       "      <td>2015-08-18 02:30:07</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _score          _index                                            _source  \\\n",
       "0     391  hashtag_tweets  {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
       "1     433  hashtag_tweets  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
       "2     376  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
       "3     120  hashtag_tweets  {'tweet': {'hashtags': ['authentic', 'LaughOut...   \n",
       "4    1021  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x2c91...   \n",
       "\n",
       "            _crawldate   _type  tweet_id  \\\n",
       "0  2015-05-23 11:42:47  tweets  0x376b20   \n",
       "1  2016-01-28 04:52:09  tweets  0x2d5350   \n",
       "2  2016-01-24 23:53:05  tweets  0x1cd5b0   \n",
       "3  2015-06-11 04:44:05  tweets  0x1d755c   \n",
       "4  2015-08-18 02:30:07  tweets  0x2c91a8   \n",
       "\n",
       "                                                text  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "3  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "4       Still waiting on those supplies Liscus. <LH>   \n",
       "\n",
       "                       hash_tags       emotion  emotion_label  \n",
       "0                     [Snapchat]  anticipation              1  \n",
       "1  [freepress, TrumpLegacy, CNN]       sadness              5  \n",
       "2                             []          fear              3  \n",
       "3      [authentic, LaughOutLoud]           joy              4  \n",
       "4                             []  anticipation              1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df_twitter_train[\"emotion_label\"] = label_encoder.fit_transform(df_twitter_train[\"emotion\"])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "print(num_classes)\n",
    "print(df_twitter_train[\"emotion_label\"].unique())\n",
    "df_twitter_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a27897bec324d72b98df4c3b28bca06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/843k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hans\\anaconda3\\envs\\DataMining\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hans\\.cache\\huggingface\\hub\\models--vinai--bertweet-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffad98f04554aecb633bea400a6bb41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad673cccdd2c4d8f8469a8f9207dc976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0292387db244860a4030d57864ef176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/558 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertweetTokenizer\n",
    "\n",
    "tokenizer = BertweetTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True) # base --> max_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split( df_twitter_train['text'],\n",
    "                                                   df_twitter_train['emotion_label'],\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾©æ•¸æ“šé›†\n",
    "max_len = 128\n",
    "train_dataset = TweetDataset(X_train, y_train, tokenizer, max_len)\n",
    "val_dataset = TweetDataset(X_val, y_val, tokenizer, max_len)\n",
    "\n",
    "# å®šç¾©æ•¸æ“šè¼‰å…¥å™¨\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989d57db35224452aaaf2a09df3b3f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72779/72779 [3:33:27<00:00,  5.68it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.9400, Accuracy: 0.6592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72779/72779 [3:34:02<00:00,  5.67it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.8021, Accuracy: 0.7094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72779/72779 [3:46:53<00:00,  5.35it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.7064, Accuracy: 0.7445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "          f\"Accuracy: {train_correct/len(train_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to bertweet_emotion_model.pt\n"
     ]
    }
   ],
   "source": [
    "save_path = \"bertweet_emotion_model.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "# å»ºç«‹æ¸¬è©¦æ•¸æ“šé›†\n",
    "test_dataset = TestDataset(df_twitter_test['text'], tokenizer, max_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25749/25749 [19:45<00:00, 21.71it/s]\n",
      "C:\\Users\\hans\\AppData\\Local\\Temp\\ipykernel_13648\\373012705.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_twitter_test['emotion'] = label_encoder.inverse_transform(predictions)\n"
     ]
    }
   ],
   "source": [
    "# é æ¸¬æ¨¡åž‹\n",
    "model.eval()  # åˆ‡æ›ç‚ºè©•ä¼°æ¨¡å¼\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():  # ç¦æ­¢è¨ˆç®—æ¢¯åº¦ä»¥åŠ é€ŸæŽ¨è«–\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)  # ç²å–æ¯æ¢æ–‡æœ¬çš„é æ¸¬åˆ†é¡ž\n",
    "        predictions.extend(preds.cpu().numpy())  # æ”¶é›†é æ¸¬çµæžœ\n",
    "\n",
    "# å°‡æ¨™ç±¤è½‰æ›ç‚ºåŽŸå§‹æƒ…æ„Ÿåˆ†é¡žåç¨±\n",
    "df_twitter_test['emotion'] = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "submission = df_twitter_test[['tweet_id', 'emotion']]\n",
    "submission = submission.rename(columns={'tweet_id': 'id'})\n",
    "\n",
    "submission.to_csv(\"submission2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
