{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbconvert in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (7.16.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbconvert) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbconvert) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbconvert) (3.1.4)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbconvert) (5.7.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbconvert) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbconvert) (2.1.3)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbconvert) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbconvert) (5.10.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbconvert) (24.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbconvert) (1.5.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbconvert) (2.18.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbconvert) (1.3.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbconvert) (5.14.3)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from bleach!=5.0.0->nbconvert) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from bleach!=5.0.0->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from jupyter-core>=4.7->nbconvert) (4.3.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from jupyter-core>=4.7->nbconvert) (306)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbclient>=0.5.0->nbconvert) (8.6.3)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbformat>=5.7->nbconvert) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from nbformat>=5.7->nbconvert) (4.23.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from beautifulsoup4->nbconvert) (2.6)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.9.0.post0)\n",
      "Requirement already satisfied: pyzmq>=23.0 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\hans\\anaconda3\\envs\\datamining\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.4.1)\n"
     ]
    }
   ],
   "source": [
    "pip install nbconvert PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertweetTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment = 1 # Kaggle\n",
    "Environment = 2 # Local\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if Environment == 1:\n",
    "    folder_name = '/kaggle/input/dm-2024-isa-5810-lab-2-homework'\n",
    "elif Environment == 2:\n",
    "    folder_name = 'dm-2024-isa-5810-lab-2-homework'\n",
    "\n",
    "\"\"\"Load data from local or Kaggle\"\"\"\n",
    "#===========================================================================\n",
    "data_identification = pd.read_csv(folder_name + '/data_identification.csv')\n",
    "emotion = pd.read_csv(folder_name + '/emotion.csv')\n",
    "sample_submission = pd.read_csv(folder_name + '/sampleSubmission.csv')\n",
    "df_twitter = pd.read_json(folder_name + '/tweets_DM.json', lines=True)\n",
    "\n",
    "\"\"\"Seperate ids of training and testing dataset\"\"\"\n",
    "#===========================================================================\n",
    "train_ids_df = data_identification[data_identification['identification'] == 'train'].drop(['identification'], axis=1)\n",
    "test_ids_df = data_identification[data_identification['identification'] == 'test'].drop(['identification'], axis=1)\n",
    "\n",
    "\"\"\"Process the json file (tweet), inspired from internet\"\"\"\n",
    "#===========================================================================\n",
    "df_twitter_expanded = pd.json_normalize(df_twitter['_source'])\n",
    "\n",
    "df_twitter = df_twitter.drop(['_source'], axis=1)\n",
    "df_twitter = pd.concat([df_twitter, df_twitter_expanded], axis=1)\n",
    "\n",
    "\"\"\"Merge json file and emotion, ids\"\"\"\n",
    "#===========================================================================\n",
    "df_training = pd.merge(train_ids_df, emotion, on='tweet_id', how='inner')\n",
    "df_training = pd.merge(df_training, df_twitter, left_on='tweet_id', right_on='tweet.tweet_id', how='inner')\n",
    "\n",
    "df_test = pd.merge(test_ids_df, df_twitter, left_on='tweet_id', right_on='tweet.tweet_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>tweet.hashtags</th>\n",
       "      <th>tweet.tweet_id</th>\n",
       "      <th>tweet.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>joy</td>\n",
       "      <td>809</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-01-17 03:07:03</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x29e452</td>\n",
       "      <td>Huge Respectüñí @JohnnyVegasReal talking about l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>joy</td>\n",
       "      <td>808</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-07-02 09:34:06</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[spateradio, app]</td>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>trust</td>\n",
       "      <td>16</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-08-15 18:18:39</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>@KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>joy</td>\n",
       "      <td>768</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-02-11 08:49:46</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[PUBG, GamersUnite, twitch, BeHealthy, StayPos...</td>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>Come join @ambushman27 on #PUBG while he striv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>70</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-11-23 05:37:10</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[strength, bones, God]</td>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>@fanshixieen2014 Blessings!My #strength little...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       emotion  _score          _index           _crawldate  \\\n",
       "0  0x29e452           joy     809  hashtag_tweets  2015-01-17 03:07:03   \n",
       "1  0x2b3819           joy     808  hashtag_tweets  2016-07-02 09:34:06   \n",
       "2  0x2a2acc         trust      16  hashtag_tweets  2016-08-15 18:18:39   \n",
       "3  0x2a8830           joy     768  hashtag_tweets  2017-02-11 08:49:46   \n",
       "4  0x20b21d  anticipation      70  hashtag_tweets  2016-11-23 05:37:10   \n",
       "\n",
       "    _type                                     tweet.hashtags tweet.tweet_id  \\\n",
       "0  tweets                                                 []       0x29e452   \n",
       "1  tweets                                  [spateradio, app]       0x2b3819   \n",
       "2  tweets                                                 []       0x2a2acc   \n",
       "3  tweets  [PUBG, GamersUnite, twitch, BeHealthy, StayPos...       0x2a8830   \n",
       "4  tweets                             [strength, bones, God]       0x20b21d   \n",
       "\n",
       "                                          tweet.text  \n",
       "0  Huge Respectüñí @JohnnyVegasReal talking about l...  \n",
       "1  Yoooo we hit all our monthly goals with the ne...  \n",
       "2  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...  \n",
       "3  Come join @ambushman27 on #PUBG while he striv...  \n",
       "4  @fanshixieen2014 Blessings!My #strength little...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>tweet.hashtags</th>\n",
       "      <th>tweet.tweet_id</th>\n",
       "      <th>tweet.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>107</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-01-17 14:13:32</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>@Habbo I've seen two separate colours of the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>728</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-10-17 06:46:20</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>@FoxNews @KellyannePolls No serious self respe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>491</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-12-19 03:50:27</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[womendrivers]</td>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>Looking for a new car, and it says 1 lady owne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x23f9e9</td>\n",
       "      <td>28</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-04-09 19:32:19</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[robbingmembers]</td>\n",
       "      <td>0x23f9e9</td>\n",
       "      <td>@cineworld ‚Äúonly the brave‚Äù just out and fount...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1fb4e1</td>\n",
       "      <td>925</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-15 11:59:31</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x1fb4e1</td>\n",
       "      <td>Felt like total dog üí© going into open gym and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id  _score          _index           _crawldate   _type  \\\n",
       "0  0x28cc61     107  hashtag_tweets  2017-01-17 14:13:32  tweets   \n",
       "1  0x2db41f     728  hashtag_tweets  2015-10-17 06:46:20  tweets   \n",
       "2  0x2466f6     491  hashtag_tweets  2016-12-19 03:50:27  tweets   \n",
       "3  0x23f9e9      28  hashtag_tweets  2017-04-09 19:32:19  tweets   \n",
       "4  0x1fb4e1     925  hashtag_tweets  2016-01-15 11:59:31  tweets   \n",
       "\n",
       "     tweet.hashtags tweet.tweet_id  \\\n",
       "0                []       0x28cc61   \n",
       "1                []       0x2db41f   \n",
       "2    [womendrivers]       0x2466f6   \n",
       "3  [robbingmembers]       0x23f9e9   \n",
       "4                []       0x1fb4e1   \n",
       "\n",
       "                                          tweet.text  \n",
       "0  @Habbo I've seen two separate colours of the e...  \n",
       "1  @FoxNews @KellyannePolls No serious self respe...  \n",
       "2  Looking for a new car, and it says 1 lady owne...  \n",
       "3  @cineworld ‚Äúonly the brave‚Äù just out and fount...  \n",
       "4  Felt like total dog üí© going into open gym and ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3295776"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_training['emotion_label'] = label_encoder.fit_transform(df_training['emotion'])\n",
    "num_classes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertweetTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True) # base --> max_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df_training['tweet.text'],\n",
    "                                                  df_training['emotion_label'],\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_len = 128\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders\n",
    "train_dataset = TweetDataset(X_train, y_train, tokenizer, max_len)\n",
    "val_dataset = TweetDataset(X_val, y_val, tokenizer, max_len)\n",
    "\n",
    "test_dataset = TweetDataset(df_test['tweet.text'], [0] * len(df_test), tokenizer, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\hans\\anaconda3\\envs\\DataMining\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"vinai/bertweet-base\",\n",
    "    num_labels=num_classes\n",
    ")\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epochs)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72779/72779 [2:57:27<00:00,  6.84it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Training  - Loss: 1.0313, Accuracy: 0.6263\n",
      "  Validation - Loss: 0.8746, Accuracy: 0.6817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72779/72779 [2:54:59<00:00,  6.93it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "  Training  - Loss: 0.7832, Accuracy: 0.7162\n",
      "  Validation - Loss: 0.8442, Accuracy: 0.6978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72779/72779 [2:58:15<00:00,  6.80it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "  Training  - Loss: 0.6575, Accuracy: 0.7619\n",
      "  Validation - Loss: 0.8712, Accuracy: 0.7004\n",
      "Model saved as bertweet_emotion_model.pth\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_accuracy = train_correct / len(train_dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_correct += (logits.argmax(1) == labels).sum().item()\n",
    "            all_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "\n",
    "    val_accuracy = val_correct / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"  Training  - Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"  Validation - Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"bertweet_emotion_model.pth\")\n",
    "print(\"Model saved as bertweet_emotion_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\hans\\AppData\\Local\\Temp\\ipykernel_30528\\610355408.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"bertweet_emotion_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready for prediction.\n"
     ]
    }
   ],
   "source": [
    "# Âä†ËºâÊ®°Âûã\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"vinai/bertweet-base\",\n",
    "    num_labels=num_classes\n",
    ")\n",
    "model.load_state_dict(torch.load(\"bertweet_emotion_model.pth\"))\n",
    "model.to(device)\n",
    "model.eval()  # Ë®≠ÁΩÆÁÇ∫Ë©ï‰º∞Ê®°Âºè\n",
    "print(\"Model loaded and ready for prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTweetDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = list(texts)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestTweetDataset(texts=df_test['tweet.text'], tokenizer=tokenizer, max_len=max_len)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25749/25749 [20:26<00:00, 20.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        test_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "\n",
    "# Convert predictions back to emotions\n",
    "df_test['emotion'] = label_encoder.inverse_transform(test_preds)\n",
    "\n",
    "submission = df_test[['tweet_id', 'emotion']]\n",
    "submission = submission.rename(columns={'tweet_id': 'id'})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
